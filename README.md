# Application-of-Principal-Component-Analysis-to-Interest-Rates
## Introduction

I chose to apply the statistical process of Principal Component Analysis to analyze the US treasury market. Principal Component Analysis aims to reduce the dimensionality and compress data in situations where one may have many closely related variables. Principle Component Analysis will perform a linear transformation on the data so that most of the variance of our high dimension set of interest rates is captured in the first few principal components. Additionally, it is helpful to utilize PCA to visualize your data in easier ways, considering there are now fewer variables to work with. Visualizing the data becomes much more appealing and easier to work with. One of the features of PCA is that important information is extracted in the form of components. The first component will capture the maximum variance in the data set, and then determines the direction of the highest variability in the data. In our case, each of our US Treasuries have quite a high correlation with each other. The PCA model will decompose the structure of our set of rates into factors that are common. It is known that PCA is typically most useful where the variables are closely related, and as mentioned in our case the rates are highly correlated as shown by our correlation matrix. I aim to extract important information in the form of components. A final goal of the project is to determine whether these interest rates truly are closely related. I would next like to introduce our specific project and discuss how I chose to utilize Principal Component Analysis to analyze a set of US Treasury Rates.

## PCA Analysis

As I briefly mentioned before, I decided to download the data for 30 individual US treasury zero-coupon bond rates over a period of roughly 37 years from 1985 until 2022. One of our first steps was to obtain the data online and export it into excel so I can observe and clean the data. I utilized the Federal Reserve website, which tracks many historical interest rates When I downloaded the set of data, I changed some of the column names and cleaned the rows so that python would easily recognize the data set. After loading the data into python, I dropped any NAN values that may have been recorded due to Holidays or weekends and plotted the data. With just another line of code I were able to run a correlation matrix between each of the 30 interest rates. This allowed us to quickly determine if there was initially a statistical correlation between any of the interest rates. The interest rates that were very close to one another, for example the 29-year and 30-year interest rates, had a very high correlation of over 95%. Moreover, looking at interest rates that were very spread apart, such as the 1-year interest rate versus the 30-year interest rate, I found a much lower correlation of closer to 80%.

The high correlation between the interest rates supports the PCA model to reduce dimensionality, as the variance of the data points projection onto the first principal component will be
much larger than other principal components, the first principal component will be able to explain variance well. To confirm this, I performed the PCA model to reduce the dimensionality from 30 to 3 and made a scree plot to visualize how much variance each principal component could explain. The first component had the maximum variance of the projection, so it always has the highest explained variance ratio. In this case, the first component had a proportion of variance explained, or PVE of 0.97, the second component had a PVE of 0.026, and the third component had a PVE of 0.0015. Moreover, I made a cumulative scree plot to show the cumulative PVE by the 3 components. The first component had a cumulative PVE of 0.97, the first two components had a PVE of 0.997, and the 3 components have a cumulative PVE of 0.999. Although the 3 components could explain 0.999 of the variance, they still could not explain all the variance. I performed the PCA model to obtain the 30 principal components (all the principal components) to exam if they could have a PVE of 1. I used a for loop in python to calculate the cumulative PVE of the 30 principal components, and the result was 1, which was the same as I expected.

To better interpret the 3 principal components, I made a plot of them. It was easier to see that each component has 30 coefficient (30 points in each line). Each point represents a coefficient to a corresponding variable, the absolute coefficient values show how important the corresponding variable is to the principal component. For instance, the value of the first blue point on the left side is 0.195, which is the coefficient of the first variable (interest rate) in the first principal component. The first variable of the first principal component is “1 Year Zero Coupon Bond”. Therefore, when the 1 Year Zero Coupon Bond goes up 1 percent, the first component goes up 0.195 percent. Once the coefficients of the components were available, the principal component scores could be calculated. First, centered the data, meaning subtracting the mean of a variable from each data point in that variable. Since each component was a linear combination, a principal component value was the sum of each centered data point multiplying by its coefficient. There were three principal components and 9084 samples, so a 9084*3 matrix (or a dataframe in python) can be derived. I then made a plot of the principal component scores, the first principal component was similar to the original data due to its high PVE, whereas the second and the third principal components were completely different.

Once the principal components scores were calculated, I restored them. The dimensionality increased from 3 to 30, the restored data were similar to the original data due to the PVE of the three components was 99%. They would be the same only if the components could explain 100% of the variance. As a result, the 30 interest rates were closely correlated and the dimensionality could be reduced from 30 to 3 without losing much information.

## Limitations

Like every model, PCA analysis also has some drawbacks. For examples, it is hard to interpret the components I get from the model. As per our results discussed previously in this report, almost all variance can be captured with three components. When dealing with interest rates, I learned that the three components I found are actually well known. Components 1 to 3 are named “Shift”, “Twist” and “Butterfly” respectively. Their effect on the yield curves is also known and it helps to understand the meaning of those components. However, results can be much more abstract when applying PCA outside of interest rates. To understand and interpret those components becomes much more challenging.

Another disadvantage is that your components might not be suited for the analysis you are trying to make. As I mentioned, PCA reduces dimensionality and gives us a new data that is much easier to work with. However, it is possible that the principal element of your new data is not incorporated in the components. Therefore, you might end up performing an analysis with data that is now incomplete and have your results biased.

Finally, it is important to know that PCA is not robust against outliers. Extreme variables will end up over-represented and distort the results of your model. As a result, unit of measurement needs to be similar across your data. If variable X1 is scaled between 0 and 1 while variable X2 is scaled between 0 and 1000, variable X2 weight will be disproportionate.

## Possible Application of PCA

When working on this project, I came across different PCA implementations. I focused on reducing dimensionality and understanding how PCA models work. Using more time and resources, I could have explored those different uses. For example, I learned that PCA can be used in investments strategies and hedging. Doing so, would have added a more practical use to our work and expand our understanding of the subject. I could have also tried to perform PCA on different data. Run different analysis on the compressed data and compare it with the original one to compare our findings.
